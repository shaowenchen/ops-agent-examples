# Ops Agent SLO

ä¸€ä¸ªç”¨äºæœåŠ¡çº§åˆ«ç›®æ ‡ï¼ˆSLOï¼‰ç›‘æ§å’Œåˆ†æçš„å·¥å…·ï¼Œé€šè¿‡ä»£ç ç¼–æ’å„ä¸ªæ£€æŸ¥æ¨¡å—ï¼Œä¼ é€’å’Œå¤ç”¨å‚æ•°ï¼Œè°ƒç”¨ MCP æˆ– LLM æœåŠ¡ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ”§ **æ¨¡å—åŒ–è®¾è®¡**: æ¯ä¸ªæ£€æŸ¥æ¨¡å—ç‹¬ç«‹ç›®å½•ï¼Œæ˜“äºæ‰©å±•å’Œç»´æŠ¤
- ğŸ”„ **å‚æ•°ä¼ é€’å’Œå¤ç”¨**: æ¨¡å—é—´é€šè¿‡å…±äº«ä¸Šä¸‹æ–‡ä¼ é€’å‚æ•°å’Œæ•°æ®
- ğŸ¯ **å·¥ä½œæµç¼–æ’**: æ”¯æŒå¤æ‚çš„å·¥ä½œæµå®šä¹‰ï¼ŒåŒ…æ‹¬æ¡ä»¶æ‰§è¡Œå’Œç»“æœå¤ç”¨
- ğŸ”Œ **MCP é›†æˆ**: æ”¯æŒè°ƒç”¨å¤šä¸ª MCP æœåŠ¡å™¨
- ğŸ¤– **LLM æ”¯æŒ**: å®Œæ•´çš„ LLM äº¤äº’æ¨¡å—ï¼Œæ”¯æŒ WPS AI Gateway å’Œå…¶ä»– LLM æä¾›å•†ï¼Œå¯ç”¨äºæ™ºèƒ½åˆ†æå’Œå†³ç­–
- ğŸŒ **HTTP API**: æä¾› HTTP æœåŠ¡æ¥å£ï¼Œæ”¯æŒé€šè¿‡ API è§¦å‘å·¥ä½œæµ
- ğŸ“Š **ç»“æœå¯è§†åŒ–**: ä½¿ç”¨ Rich åº“æä¾›ç¾è§‚çš„æ§åˆ¶å°è¾“å‡º

## é¡¹ç›®ç»“æ„

```
ops-agent-slo/
â”œâ”€â”€ main.py                 # ä¸»ç¨‹åºå…¥å£ï¼ˆæ‰§è¡Œ run.pyï¼‰
â”œâ”€â”€ run.py                  # ä¸»è¦ä»£ç åŒºåŸŸï¼Œç›´æ¥ç¼–å†™æ¨¡å—ç»„åˆä»£ç 
â”œâ”€â”€ server.py               # HTTP API æœåŠ¡
â”œâ”€â”€ Dockerfile              # Docker é•œåƒæ„å»ºæ–‡ä»¶
â”œâ”€â”€ .dockerignore           # Docker å¿½ç•¥æ–‡ä»¶
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml        # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt        # Python ä¾èµ–
â”œâ”€â”€ README.md              # è¯´æ˜æ–‡æ¡£
â””â”€â”€ ops_agent/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ config/             # é…ç½®æ¨¡å—
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ config_loader.py
    â”œâ”€â”€ core/               # æ ¸å¿ƒæ¨¡å—
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ orchestrator.py # ç¼–æ’å™¨
    â”‚   â””â”€â”€ base_module.py  # æ¨¡å—åŸºç±»
    â”œâ”€â”€ modules/            # æ£€æŸ¥æ¨¡å—
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ upstream_query/ # Upstream æŸ¥è¯¢æ¨¡å—
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ module.py
    â”‚   â””â”€â”€ error_log_query/ # å¼‚å¸¸æ—¥å¿—æŸ¥è¯¢æ¨¡å—
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â””â”€â”€ module.py
    â”‚   â””â”€â”€ llm_chat/         # LLM äº¤äº’æ¨¡å—
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â””â”€â”€ module.py
    â”œâ”€â”€ tools/              # å·¥å…·æ¨¡å—
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ mcp_tool.py     # MCP å·¥å…·å°è£…
    â””â”€â”€ utils/              # å·¥å…·æ¨¡å—
        â”œâ”€â”€ __init__.py
        â””â”€â”€ logging.py
```

## å®‰è£…

1. è¿›å…¥é¡¹ç›®ç›®å½•ï¼š
```bash
cd ops-agent-slo
```

2. å®‰è£…ä¾èµ–ï¼š
```bash
pip install -r requirements.txt
```

## é…ç½®

### 1. é…ç½®æ–‡ä»¶

ç¼–è¾‘ `configs/config.yaml` æ–‡ä»¶ï¼Œé…ç½® MCP æœåŠ¡å™¨å’Œ LLMï¼š

```yaml
# Ops Agent SLO Configuration

# MCP Server Configuration
mcp_servers:
  - name: "MCP1"
    server_url: "https://your-mcp-server-url.com/mcp"
    timeout: "30s"
    token: "your-mcp-token-here"
    default: true

# LLM Configuration (optional)
llm:
  token: "your-llm-token-here"
  url: "http://your-llm-gateway.com/api/v2/llm/chat"
  provider: "azure"
  model: "gpt-4o"
  temperature: 0
  headers_json: '{"CUSTOM-HEADER-NAME-1":"value1","CUSTOM-HEADER-NAME-2":"value2","CUSTOM-HEADER-NAME-3":"value3"}'
```

### 1.1 ç¯å¢ƒå˜é‡é…ç½®

ç¯å¢ƒå˜é‡ä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ã€‚æ”¯æŒçš„ç¯å¢ƒå˜é‡ï¼š

**LLM ç›¸å…³ï¼š**
- `LLM_TOKEN`: LLM API tokenï¼ˆè¦†ç›– config ä¸­çš„ `llm.token`ï¼‰
- `LLM_URL`: LLM API URLï¼ˆè¦†ç›– config ä¸­çš„ `llm.url`ï¼‰
- `LLM_HEADERS_JSON`: LLM è¯·æ±‚å¤´ï¼ˆJSON å­—ç¬¦ä¸²æ ¼å¼ï¼Œè¦†ç›– config ä¸­çš„ `llm.headers_json`ï¼‰

**é…ç½®ç¤ºä¾‹ï¼š**

```bash
# è®¾ç½® LLM token
export LLM_TOKEN="your-llm-token"

# è®¾ç½® LLM URL
export LLM_URL="http://your-llm-gateway.com/api/v2/llm/chat"

# è®¾ç½® LLM headersï¼ˆJSON å­—ç¬¦ä¸²æ ¼å¼ï¼‰
export LLM_HEADERS_JSON='{"CUSTOM-HEADER-NAME-1":"value1","CUSTOM-HEADER-NAME-2":"value2","CUSTOM-HEADER-NAME-3":"value3"}'
```

**æ³¨æ„ï¼š** `LLM_HEADERS_JSON` å¿…é¡»æ˜¯æœ‰æ•ˆçš„ JSON å­—ç¬¦ä¸²ã€‚å¦‚æœåŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼Œå»ºè®®ä½¿ç”¨å•å¼•å·åŒ…è£¹æ•´ä¸ª JSON å­—ç¬¦ä¸²ã€‚

**Docker ä¸­ä½¿ç”¨ï¼š**

```bash
docker run -e LLM_TOKEN="your-token" \
  -e LLM_URL="http://your-llm-gateway.com/api/v2/llm/chat" \
  -e LLM_HEADERS_JSON='{"CUSTOM-HEADER-NAME-1":"value1","CUSTOM-HEADER-NAME-2":"value2","CUSTOM-HEADER-NAME-3":"value3"}' \
  ops-agent-slo:latest
```

**Docker Compose ä¸­ä½¿ç”¨ï¼š**

```yaml
services:
  ops-agent-slo:
    environment:
      - LLM_TOKEN=your-llm-token
      - LLM_URL=http://your-llm-gateway.com/api/v2/llm/chat
      - LLM_HEADERS_JSON={"CUSTOM-HEADER-NAME-1":"value1","CUSTOM-HEADER-NAME-2":"value2","CUSTOM-HEADER-NAME-3":"value3"}
```

### 2. ç¼–å†™æ‰§è¡Œä»£ç 

**ç›´æ¥ç¼–è¾‘ `run.py` æ–‡ä»¶æ¥ç»„åˆå’Œæ‰§è¡Œæ¨¡å—**ï¼Œä½¿ç”¨ Python ä»£ç ï¼š

```python
def main():
    # åˆå§‹åŒ–é…ç½®å’Œç¼–æ’å™¨
    config_loader = ConfigLoader()
    config_loader.load_config()
    
    orchestrator = Orchestrator(config_loader)
    
    # æ³¨å†Œæ¨¡å—
    orchestrator.register_module(UpstreamQueryModule())
    orchestrator.register_module(ErrorLogQueryModule())
    
    # ç›´æ¥è°ƒç”¨æ¨¡å—å¹¶ç»„åˆ
    # æŸ¥è¯¢ qingqiu æœåŠ¡çš„ upstream ä¿¡æ¯
    upstream_result = orchestrator.execute_module(
        "upstream_query",
        params={
            "service_name": "qingqiu",
            "mcp_server": "default"
        }
    )
    
    # ä½¿ç”¨ä¸Šä¸€ä¸ªæ¨¡å—çš„ç»“æœï¼ŒæŸ¥è¯¢å¼‚å¸¸æ—¥å¿—
    service_name = orchestrator.get_context('last_queried_service', 'qingqiu')
    
    error_log_result = orchestrator.execute_module(
        "error_log_query",
        params={
            "service_name": service_name,
            "index": "logs-*",
            "time_range": "1h"
        }
    )
    
    # å¯ä»¥æ·»åŠ æ›´å¤šé€»è¾‘ï¼Œå¦‚æ¡ä»¶åˆ¤æ–­ã€å¾ªç¯ç­‰
    if upstream_result.status.value == "success":
        # å¤„ç†ç»“æœ...
        pass
```

è¿™ç§æ–¹å¼å®Œå…¨ä½¿ç”¨ Python ä»£ç ï¼Œå¯ä»¥è‡ªç”±ç»„åˆæ¨¡å—ã€ä¼ é€’å‚æ•°ã€å¤ç”¨ç»“æœï¼Œæ”¯æŒæ‰€æœ‰ Python åŠŸèƒ½ã€‚

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ç”¨æ³•

ç›´æ¥è¿è¡Œä¸»ç¨‹åºï¼Œå®ƒä¼šæ‰§è¡Œ `run.py` ä¸­å®šä¹‰çš„æ¨¡å—ç»„åˆé€»è¾‘ï¼š

```bash
python main.py
```

æˆ–è€…ç›´æ¥è¿è¡Œï¼š

```bash
python run.py
```

### é»˜è®¤æ‰§è¡Œæµç¨‹

é»˜è®¤æƒ…å†µä¸‹ï¼Œ`run.py` ä¼šæ‰§è¡Œä»¥ä¸‹æµç¨‹ï¼š

1. **æŸ¥è¯¢ Upstream ä¿¡æ¯** - æŸ¥è¯¢æŒ‡å®šæœåŠ¡çš„ upstream é…ç½®
2. **æŸ¥è¯¢é”™è¯¯æ—¥å¿—** - åŸºäºæœåŠ¡åç§°æŸ¥è¯¢å¼‚å¸¸æ—¥å¿—
3. **LLM æ™ºèƒ½åˆ†æ** - ä½¿ç”¨ LLM æ¨¡å—åˆ†æå‰é¢ä¸¤ä¸ªæ¨¡å—çš„ç»“æœï¼Œç”Ÿæˆåˆ†ææŠ¥å‘Š

### æ‰§è¡Œç¤ºä¾‹è¾“å‡º

```
Upstream query result: success
Service: qingqiu
Upstreams found: 5

Error log query result: success
Total errors: 12

================================================================================
è°ƒç”¨ LLM æ¨¡å—è¿›è¡Œåˆ†æ...
================================================================================

âœ… LLM åˆ†æç»“æœ:
--------------------------------------------------------------------------------
æ ¹æ®ç›‘æ§æ•°æ®åˆ†æï¼ŒæœåŠ¡ qingqiu å½“å‰çŠ¶æ€å¦‚ä¸‹ï¼š

1. Upstream é…ç½®æ­£å¸¸ï¼Œå…±å‘ç° 5 ä¸ª upstream èŠ‚ç‚¹
2. å‘ç° 12 æ¡é”™è¯¯æ—¥å¿—ï¼Œå»ºè®®è¿›ä¸€æ­¥æ’æŸ¥...

Token ä½¿ç”¨æƒ…å†µ: 256 tokens
```

### ç¼–è¾‘æ‰§è¡Œä»£ç 

ç›´æ¥ç¼–è¾‘ `run.py` æ–‡ä»¶ï¼Œåœ¨ `main()` å‡½æ•°ä¸­ç¼–å†™ä½ çš„ä»£ç æ¥ç»„åˆæ¨¡å—ï¼š

```python
def main():
    # åˆå§‹åŒ–é…ç½®å’Œç¼–æ’å™¨
    config_loader = ConfigLoader()
    config_loader.load_config()
    
    orchestrator = Orchestrator(config_loader)
    
    # æ³¨å†Œæ¨¡å—
    orchestrator.register_module(UpstreamQueryModule())
    orchestrator.register_module(ErrorLogQueryModule())
    orchestrator.register_module(LLMChatModule())
    
    # ç›´æ¥è°ƒç”¨æ¨¡å—å¹¶ç»„åˆ
    # ä½ çš„ä»£ç ...
```

### è¯¦ç»†æ—¥å¿—

```bash
python main.py --verbose
```

## Docker éƒ¨ç½²

### æ„å»ºé•œåƒ

```bash
docker build -t ops-agent-slo:latest .
```

### è¿è¡Œå®¹å™¨

```bash
# è¿è¡Œ HTTP æœåŠ¡
docker run -d \
  -p 8080:8080 \
  -v $(pwd)/configs:/app/configs:ro \
  -v $(pwd)/run.py:/app/run.py:rw \
  -e MCP_SERVERS_JSON='[{"name":"MCP1","server_url":"...","token":"..."}]' \
  -e LLM_TOKEN="your-llm-token" \
  -e LLM_URL="http://your-llm-gateway.com/api/v2/llm/chat" \
  -e LLM_HEADERS_JSON='{"CUSTOM-HEADER-NAME-1":"value1","CUSTOM-HEADER-NAME-2":"value2","CUSTOM-HEADER-NAME-3":"value3"}' \
  ops-agent-slo:latest

# æˆ–è€…ç›´æ¥æ‰§è¡Œ main.pyï¼ˆä¸€æ¬¡æ€§æ‰§è¡Œï¼‰
docker run --rm \
  -v $(pwd)/configs:/app/configs:ro \
  -v $(pwd)/run.py:/app/run.py:rw \
  -e MCP_SERVERS_JSON='[{"name":"MCP1","server_url":"...","token":"..."}]' \
  -e LLM_TOKEN="your-llm-token" \
  -e LLM_HEADERS_JSON='{"CUSTOM-HEADER-NAME-1":"value1","CUSTOM-HEADER-NAME-2":"value2","CUSTOM-HEADER-NAME-3":"value3"}' \
  ops-agent-slo:latest python main.py
```

### ä½¿ç”¨ Docker Compose

åˆ›å»º `docker-compose.yml`:

```yaml
version: '3.8'

services:
  ops-agent-slo:
    build: .
    ports:
      - "8080:8080"
    volumes:
      - ./configs:/app/configs:ro
      - ./run.py:/app/run.py:rw
    environment:
      - MCP_SERVERS_JSON=[{"name":"MCP1","server_url":"...","token":"..."}]
      - LLM_TOKEN=your-llm-token
      - LLM_URL=http://your-llm-gateway.com/api/v2/llm/chat
      - LLM_HEADERS_JSON={"CUSTOM-HEADER-NAME-1":"value1","CUSTOM-HEADER-NAME-2":"value2","CUSTOM-HEADER-NAME-3":"value3"}
    restart: unless-stopped
```

è¿è¡Œï¼š
```bash
docker-compose up -d
```

## HTTP API æœåŠ¡

### å¯åŠ¨æœåŠ¡

```bash
python server.py
```

æœåŠ¡é»˜è®¤è¿è¡Œåœ¨ `http://0.0.0.0:8080`

### API ç«¯ç‚¹

#### 1. å¥åº·æ£€æŸ¥

**GET** `/health`

æ£€æŸ¥æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œã€‚

#### 2. è§¦å‘å·¥ä½œæµ

**GET/POST** `/trigger`

è§¦å‘æ‰§è¡Œåˆ†ææµç¨‹ã€‚æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. å¦‚æœæä¾› `data` å’Œ `key` å‚æ•°ï¼Œæ‰§è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
2. å¦‚æœä¸æä¾›ï¼Œæ‰§è¡Œ `run.py` ä¸­çš„é»˜è®¤ä»£ç 

**è¯·æ±‚ç¤ºä¾‹ï¼ˆGETï¼‰ï¼š**
```
GET /trigger?verbose=false
```

**è¯·æ±‚ç¤ºä¾‹ï¼ˆPOST - å¸¦ data å’Œ keyï¼‰ï¼š**
```json
{
  "data": "åˆ†æå†…å®¹æˆ–æ•°æ®",
  "key": "æ ‡è¯†é”®ï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„åˆ†æç±»å‹",
  "verbose": false
}
```

**è¯·æ±‚ç¤ºä¾‹ï¼ˆPOST - ä¸å¸¦å‚æ•°ï¼Œæ‰§è¡Œ run.pyï¼‰ï¼š**
```json
{
  "verbose": false
}
```

**å‚æ•°è¯´æ˜ï¼š**
- `data` (å¯é€‰): åˆ†æå†…å®¹æˆ–æ•°æ®ï¼Œä¼šä¼ é€’ç»™ LLM è¿›è¡Œåˆ†æ
- `key` (å¯é€‰): æ ‡è¯†é”®ï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„åˆ†æç±»å‹
  - å¦‚æœ `key` ä»¥ `service_` å¼€å¤´ï¼Œä¼šè‡ªåŠ¨æå–æœåŠ¡åç§°å¹¶æ‰§è¡Œå®Œæ•´çš„ç›‘æ§åˆ†ææµç¨‹
  - å…¶ä»– `key` å€¼ä¼šç›´æ¥ä½¿ç”¨ LLM åˆ†æ `data` å†…å®¹
- `verbose` (å¯é€‰): æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼Œé»˜è®¤ false

**å“åº”ç¤ºä¾‹ï¼ˆå¸¦ data å’Œ keyï¼‰ï¼š**
```json
{
  "success": true,
  "key": "service_qingqiu",
  "results": {
    "upstream": {
      "module_name": "upstream_query",
      "status": "success",
      "data": {...}
    },
    "error_log": {
      "module_name": "error_log_query",
      "status": "success",
      "data": {...}
    },
    "llm_analysis": {
      "module_name": "llm_chat",
      "status": "success",
      "data": {
        "output": "åˆ†æç»“æœ...",
        "usage": {...}
      }
    }
  },
  "context": {...}
}
```

**å“åº”ç¤ºä¾‹ï¼ˆä¸å¸¦å‚æ•°ï¼‰ï¼š**
```json
{
  "success": true,
  "results": {
    "output": "æ‰§è¡Œè¾“å‡º..."
  },
  "context": {...}
}
```

**åˆ†ææµç¨‹ï¼ˆå½“æä¾› data å’Œ key æ—¶ï¼‰ï¼š**
1. å¦‚æœ `key` ä»¥ `service_` å¼€å¤´ï¼š
   - æå–æœåŠ¡åç§°ï¼ˆä» key ä¸­ï¼Œå¦‚ `service_qingqiu` â†’ `qingqiu`ï¼‰
   - æ‰§è¡Œ upstream æŸ¥è¯¢
   - æ‰§è¡Œé”™è¯¯æ—¥å¿—æŸ¥è¯¢
   - å°†æŸ¥è¯¢ç»“æœå’Œæ¥æ”¶åˆ°çš„ `data` ä¸€èµ·ä¼ é€’ç»™ LLM è¿›è¡Œç»¼åˆåˆ†æ
2. å…¶ä»–æƒ…å†µï¼š
   - ç›´æ¥ä½¿ç”¨ LLM åˆ†ææ¥æ”¶åˆ°çš„ `data` å†…å®¹

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```bash
# æœåŠ¡åˆ†æï¼ˆå®Œæ•´æµç¨‹ï¼‰
curl -X POST http://localhost:8080/trigger \
  -H "Content-Type: application/json" \
  -d '{
    "data": "è¯·åˆ†ææœåŠ¡ qingqiu çš„å¥åº·çŠ¶å†µ",
    "key": "service_qingqiu"
  }'

# ç›´æ¥ LLM åˆ†æ
curl -X POST http://localhost:8080/trigger \
  -H "Content-Type: application/json" \
  -d '{
    "data": "åˆ†æè¿™æ®µæ—¥å¿—æ•°æ®...",
    "key": "log_analysis"
  }'

# æ‰§è¡Œ run.py é»˜è®¤æµç¨‹
curl -X POST http://localhost:8080/trigger \
  -H "Content-Type: application/json" \
  -d '{}'
```

## æ¨¡å—å¼€å‘

### åˆ›å»ºæ–°æ¨¡å—

1. åœ¨ `ops_agent/modules/` ä¸‹åˆ›å»ºæ–°ç›®å½•ï¼Œä¾‹å¦‚ `my_module/`
2. åˆ›å»º `__init__.py` å’Œ `module.py`
3. ç»§æ‰¿ `BaseModule` å¹¶å®ç° `execute()` æ–¹æ³•

ç¤ºä¾‹ï¼š

```python
from ...core.base_module import BaseModule, ModuleResult, ModuleStatus
from ...utils.logging import get_logger

logger = get_logger(__name__)

class MyModule(BaseModule):
    def __init__(self, mcp_tool=None, context=None):
        super().__init__("my_module", mcp_tool, context)
    
    def execute(self, params):
        # è·å–å‚æ•°
        service_name = params.get('service_name')
        
        # ä»ä¸Šä¸‹æ–‡è·å–å€¼
        previous_result = self.get_context_value('upstream_query_result')
        
        # è°ƒç”¨ MCP å·¥å…·
        result = self.call_mcp_tool(
            tool_name="some-tool",
            args={"key": "value"},
            server_name="default"
        )
        
        # è®¾ç½®ä¸Šä¸‹æ–‡å€¼ä¾›å…¶ä»–æ¨¡å—ä½¿ç”¨
        self.set_context_value('my_result', result)
        
        # è¿”å›ç»“æœ
        return ModuleResult(
            module_name=self.name,
            status=ModuleStatus.SUCCESS,
            data={"result": result}
        )
```

4. åœ¨ `ops_agent/modules/__init__.py` ä¸­æ³¨å†Œæ¨¡å—

5. åœ¨ `run.py` ä¸­ç›´æ¥è°ƒç”¨å’Œä½¿ç”¨

## å·¥ä½œæµç‰¹æ€§

### å‚æ•°ä¼ é€’å’Œå¤ç”¨

æ¨¡å—å¯ä»¥é€šè¿‡å…±äº«ä¸Šä¸‹æ–‡ä¼ é€’å‚æ•°å’Œå¤ç”¨ç»“æœï¼š

```python
# æ‰§è¡Œç¬¬ä¸€ä¸ªæ¨¡å—
upstream_result = orchestrator.execute_module(
    "upstream_query",
    params={"service_name": "qingqiu"}
)
# ç»“æœä¼šè‡ªåŠ¨ä¿å­˜åˆ°ä¸Šä¸‹æ–‡: upstream_query_result

# ä»ä¸Šä¸‹æ–‡è·å–æœåŠ¡åç§°
service_name = orchestrator.get_context('last_queried_service', 'qingqiu')

# ä½¿ç”¨ä¸Šä¸€ä¸ªæ¨¡å—çš„ç»“æœæ‰§è¡Œç¬¬äºŒä¸ªæ¨¡å—
error_log_result = orchestrator.execute_module(
    "error_log_query",
    params={"service_name": service_name}
)
```

### æ¡ä»¶æ‰§è¡Œ

å¯ä»¥ä½¿ç”¨ Python çš„æ¡ä»¶è¯­å¥ï¼š

```python
upstream_result = orchestrator.execute_module(
    "upstream_query",
    params={"service_name": "qingqiu"}
)

# ä»…åœ¨æˆåŠŸæ—¶æ‰§è¡Œ
if upstream_result.status.value == "success":
    error_log_result = orchestrator.execute_module(
        "error_log_query",
        params={"service_name": "qingqiu"}
    )
```

### ä½¿ç”¨ Python åŠŸèƒ½

ç”±äºæ˜¯çº¯ Python ä»£ç ï¼Œä½ å¯ä»¥ä½¿ç”¨æ‰€æœ‰ Python åŠŸèƒ½ï¼š

```python
# ä»ç¯å¢ƒå˜é‡è¯»å–
service_name = os.environ.get('SERVICE_NAME', 'qingqiu')

# å¾ªç¯å¤„ç†å¤šä¸ªæœåŠ¡
services = ['qingqiu', 'service2', 'service3']
for service in services:
    result = orchestrator.execute_module(
        "upstream_query",
        params={"service_name": service}
    )
    # å¤„ç†ç»“æœ...
    if result.status.value == "success":
        # ç»§ç»­å¤„ç†...
        pass

# ä½¿ç”¨å‡½æ•°å°è£…é€»è¾‘
def check_service(service_name):
    upstream_result = orchestrator.execute_module(
        "upstream_query",
        params={"service_name": service_name}
    )
    if upstream_result.status.value == "success":
        return orchestrator.execute_module(
            "error_log_query",
            params={"service_name": service_name}
        )
    return None
```

### ä¸Šä¸‹æ–‡è®¿é—®

æ¨¡å—å¯ä»¥é€šè¿‡ `self.get_context_value()` å’Œ `self.set_context_value()` è®¿é—®å’Œè®¾ç½®ä¸Šä¸‹æ–‡ã€‚

## ç¤ºä¾‹æ¨¡å—

### 1. Upstream Query Module

æŸ¥è¯¢æœåŠ¡çš„ upstream ä¿¡æ¯ã€‚

**å‚æ•°ï¼š**
- `service_name`: æœåŠ¡åç§°ï¼ˆå¿…éœ€ï¼‰
- `mcp_server`: MCP æœåŠ¡å™¨åç§°ï¼ˆå¯é€‰ï¼‰
- `tool_name`: MCP å·¥å…·åç§°ï¼ˆå¯é€‰ï¼‰
- `additional_args`: é¢å¤–å‚æ•°ï¼ˆå¯é€‰ï¼‰

### 2. Error Log Query Module

æŸ¥è¯¢å¼‚å¸¸æ—¥å¿—ã€‚

**å‚æ•°ï¼š**
- `service_name`: æœåŠ¡åç§°ï¼ˆå¯é€‰ï¼Œå¯ä»ä¸Šä¸‹æ–‡è·å–ï¼‰
- `index`: Elasticsearch ç´¢å¼•ï¼ˆå¯é€‰ï¼‰
- `query_body`: è‡ªå®šä¹‰æŸ¥è¯¢ä½“ï¼ˆå¯é€‰ï¼‰
- `time_range`: æ—¶é—´èŒƒå›´ï¼ˆå¯é€‰ï¼‰
- `use_context`: æ˜¯å¦ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ service_nameï¼ˆå¯é€‰ï¼‰

### 3. LLM Chat Module

ä¸ LLM æ¨¡å‹äº¤äº’ï¼Œæ”¯æŒ WPS AI Gateway å’Œå…¶ä»– LLM æä¾›å•†ã€‚å¯ç”¨äºæ™ºèƒ½åˆ†æã€å†³ç­–æ”¯æŒã€æŠ¥å‘Šç”Ÿæˆç­‰åœºæ™¯ã€‚

**å‚æ•°ï¼š**
- `input`: ç”¨æˆ·è¾“å…¥æ–‡æœ¬ï¼ˆå¿…éœ€ï¼Œå¦‚æœæœªæä¾› messagesï¼‰
- `messages`: æ¶ˆæ¯åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œinput çš„æ›¿ä»£æ–¹æ¡ˆï¼‰
- `prompt`: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
- `history`: èŠå¤©å†å²ï¼ˆå¯é€‰ï¼Œå¯ä»ä¸Šä¸‹æ–‡è·å–ï¼‰
- `token`: API tokenï¼ˆå¯é€‰ï¼Œå¯ä»é…ç½®æˆ–ç¯å¢ƒå˜é‡è¯»å–ï¼‰
- `url`: LLM API URLï¼ˆå¯é€‰ï¼Œå¯ä»é…ç½®æˆ–ç¯å¢ƒå˜é‡è¯»å–ï¼‰
- `model`: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ï¼š"gpt-4o"ï¼‰
- `provider`: æä¾›å•†åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ï¼š"azure"ï¼‰
- `temperature`: æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ï¼š0ï¼‰
- `use_context`: æ˜¯å¦ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å†å²ï¼ˆå¯é€‰ï¼Œé»˜è®¤ï¼šFalseï¼‰
- `context_key`: ä»ä¸Šä¸‹æ–‡è·å–å†å²çš„é”®åï¼ˆå¯é€‰ï¼Œé»˜è®¤ï¼š"llm_history"ï¼‰
- `headers`: è‡ªå®šä¹‰ HTTP å¤´ï¼ˆå¯é€‰ï¼Œä¼šè¦†ç›–é…ç½®å’Œç¯å¢ƒå˜é‡ï¼‰

**é…ç½®ä¼˜å…ˆçº§ï¼š**
- `params` > ç¯å¢ƒå˜é‡ > `config.yaml`
- Token: `params.token` > `LLM_TOKEN` > `config.llm.token`
- URL: `params.url` > `LLM_URL` > `config.llm.url`
- Headers: `params.headers` > `LLM_HEADERS_JSON` > `config.llm.headers_json`

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åŸºæœ¬ä½¿ç”¨ï¼ˆtoken å’Œ url ä»é…ç½®è¯»å–ï¼‰
llm_result = orchestrator.execute_module(
    "llm_chat",
    params={
        "input": "åˆ†ææœåŠ¡ qingqiu çš„å¥åº·çŠ¶å†µ",
        "prompt": "ä½ æ˜¯ä¸€ä¸ªè¿ç»´ä¸“å®¶"
    }
)

# ä½¿ç”¨ä¸Šä¸‹æ–‡å†å²è¿›è¡Œè¿ç»­å¯¹è¯
llm_result = orchestrator.execute_module(
    "llm_chat",
    params={
        "input": "ç»§ç»­åˆ†æ",
        "use_context": True,  # ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å¯¹è¯å†å²
    }
)

# ç»“åˆå…¶ä»–æ¨¡å—çš„ç»“æœè¿›è¡Œæ™ºèƒ½åˆ†æ
upstream_result = orchestrator.execute_module(
    "upstream_query",
    params={"service_name": "qingqiu"}
)

error_log_result = orchestrator.execute_module(
    "error_log_query",
    params={"service_name": "qingqiu", "time_range": "1h"}
)

# å°†å¤šä¸ªæ¨¡å—çš„ç»“æœä¼ é€’ç»™ LLM è¿›è¡Œç»¼åˆåˆ†æ
analysis_prompt = f"""è¯·åˆ†ææœåŠ¡ qingqiu çš„ç›‘æ§æƒ…å†µï¼š

1. Upstream çŠ¶æ€: {upstream_result.status.value}
   - Upstream æ•°é‡: {upstream_result.data.get('upstream_info', {}).get('summary', {}).get('total_upstreams', 0) if upstream_result.data else 0}

2. é”™è¯¯æ—¥å¿—çŠ¶æ€: {error_log_result.status.value}
   - é”™è¯¯æ€»æ•°: {error_log_result.data.get('logs', {}).get('summary', {}).get('total_errors', 0) if error_log_result.data else 0}

è¯·ç»™å‡ºç®€è¦çš„åˆ†æå’Œå»ºè®®ã€‚"""

llm_result = orchestrator.execute_module(
    "llm_chat",
    params={
        "input": analysis_prompt,
        "prompt": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¿ç»´ä¸“å®¶ï¼Œæ“…é•¿åˆ†ææœåŠ¡ç›‘æ§æ•°æ®å’Œæ•…éšœæ’æŸ¥ã€‚è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"
    }
)

if llm_result.status.value == "success":
    print(f"åˆ†æç»“æœ: {llm_result.data.get('output')}")
    print(f"Token ä½¿ç”¨: {llm_result.data.get('usage', {}).get('total_tokens', 0)}")
```

**å®Œæ•´å·¥ä½œæµç¤ºä¾‹ï¼š**

```python
# 1. æŸ¥è¯¢æœåŠ¡ä¿¡æ¯
upstream_result = orchestrator.execute_module("upstream_query", ...)

# 2. æŸ¥è¯¢é”™è¯¯æ—¥å¿—
error_log_result = orchestrator.execute_module("error_log_query", ...)

# 3. LLM æ™ºèƒ½åˆ†æ
llm_result = orchestrator.execute_module(
    "llm_chat",
    params={
        "input": f"åˆ†ææœåŠ¡ç›‘æ§æ•°æ®ï¼š{upstream_result.data} å’Œ {error_log_result.data}",
        "prompt": "ä½ æ˜¯è¿ç»´ä¸“å®¶"
    }
)

# 4. æ ¹æ® LLM åˆ†æç»“æœå†³å®šä¸‹ä¸€æ­¥æ“ä½œ
if llm_result.status.value == "success":
    analysis = llm_result.data.get('output')
    # æ ¹æ®åˆ†æç»“æœæ‰§è¡Œåç»­æ“ä½œ...
```

## è¾“å‡º

ç¨‹åºä¼šï¼š

1. åœ¨æ§åˆ¶å°æ˜¾ç¤ºå·¥ä½œæµæ‰§è¡ŒçŠ¶æ€å’Œç»“æœ
2. å°†ç»“æœä¿å­˜åˆ° `results.json` æ–‡ä»¶

## å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ï¼šæœåŠ¡å¥åº·æ£€æŸ¥å’Œåˆ†æ

```python
# run.py ä¸­çš„å®Œæ•´ç¤ºä¾‹
def main():
    setup_logging("INFO")
    
    config_loader = ConfigLoader()
    config_loader.load_config()
    
    orchestrator = Orchestrator(config_loader)
    orchestrator.set_context('config_loader', config_loader)
    
    # æ³¨å†Œæ‰€æœ‰æ¨¡å—
    orchestrator.register_module(UpstreamQueryModule())
    orchestrator.register_module(ErrorLogQueryModule())
    orchestrator.register_module(LLMChatModule())
    
    # 1. æŸ¥è¯¢ upstream ä¿¡æ¯
    upstream_result = orchestrator.execute_module(
        "upstream_query",
        params={"service_name": "qingqiu"}
    )
    
    # 2. æŸ¥è¯¢é”™è¯¯æ—¥å¿—
    service_name = orchestrator.get_context('last_queried_service', 'qingqiu')
    error_log_result = orchestrator.execute_module(
        "error_log_query",
        params={"service_name": service_name, "time_range": "1h"}
    )
    
    # 3. LLM æ™ºèƒ½åˆ†æ
    llm_input = f"""åˆ†ææœåŠ¡ {service_name}ï¼š
    - Upstream: {upstream_result.status.value}
    - é”™è¯¯æ—¥å¿—: {error_log_result.data.get('logs', {}).get('summary', {})}
    è¯·ç»™å‡ºåˆ†æå’Œå»ºè®®ã€‚"""
    
    llm_result = orchestrator.execute_module(
        "llm_chat",
        params={
            "input": llm_input,
            "prompt": "ä½ æ˜¯è¿ç»´ä¸“å®¶"
        }
    )
    
    # 4. è¾“å‡ºç»“æœ
    if llm_result.status.value == "success":
        print(f"âœ… åˆ†æå®Œæˆ: {llm_result.data.get('output')}")
```

## æ³¨æ„äº‹é¡¹

1. **MCP æœåŠ¡å™¨é…ç½®**: ç¡®ä¿ MCP æœåŠ¡å™¨å¯è®¿é—®ä¸” token æœ‰æ•ˆ
2. **LLM é…ç½®**: ç¡®ä¿é…ç½®äº† LLM token å’Œ URLï¼ˆåœ¨ `config.yaml` æˆ–ç¯å¢ƒå˜é‡ä¸­ï¼‰
3. **æ¨¡å—æ‰§è¡Œé¡ºåº**: æ¨¡å—æŒ‰ä»£ç é¡ºåºæ‰§è¡Œï¼Œå¦‚æœæŸä¸ªæ¨¡å—å¤±è´¥ï¼Œå¯ä»¥æ·»åŠ æ¡ä»¶åˆ¤æ–­å†³å®šæ˜¯å¦ç»§ç»­
4. **ä¸Šä¸‹æ–‡æ•°æ®**: æ¨¡å—é—´é€šè¿‡å…±äº«ä¸Šä¸‹æ–‡ä¼ é€’æ•°æ®ï¼Œæ³¨æ„é”®åå†²çª
5. **ç¯å¢ƒå˜é‡ä¼˜å…ˆçº§**: ç¯å¢ƒå˜é‡ä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒä½¿ç”¨
6. **LLM Token ä½¿ç”¨**: LLM æ¨¡å—ä¼šè¿”å› token ä½¿ç”¨æƒ…å†µï¼Œå¯ç”¨äºç›‘æ§å’Œæˆæœ¬æ§åˆ¶

## è®¸å¯è¯

ï¼ˆæ ¹æ®é¡¹ç›®éœ€è¦æ·»åŠ è®¸å¯è¯ä¿¡æ¯ï¼‰

